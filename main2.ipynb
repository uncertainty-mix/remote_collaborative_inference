{
 "cells": [
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import random\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torchvision\n",
    "from torch.utils.data import DataLoader, random_split, RandomSampler, Subset\n",
    "from tqdm import tqdm\n",
    "from datasets import load_dataset\n",
    "from transformers import AutoModelForImageClassification\n",
    "import matplotlib.pyplot as plt\n",
    "from torch.optim.lr_scheduler import _LRScheduler, CosineAnnealingLR, OneCycleLR, ReduceLROnPlateau, StepLR\n",
    "import timm\n",
    "from torchvision.models import ResNet50_Weights\n",
    "from timm.data.auto_augment import rand_augment_transform\n",
    "import io\n",
    "import base64\n",
    "from openai import OpenAI\n",
    "from datetime import datetime\n",
    "import copy\n",
    "import time"
   ],
   "id": "1a47e97aa933f3fb",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# 运行前需要填入这两项\n",
    "openai_api_rul = \"\"\n",
    "openai_api_key = \"\"\n",
    "# 先训练得到模型权重后，才能进行测试\n",
    "is_eval = False"
   ],
   "id": "3b9e10282405323c",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "64c2932796bc1e01",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-28T01:51:05.695266Z",
     "iopub.status.busy": "2024-12-28T01:51:05.694662Z",
     "iopub.status.idle": "2024-12-28T01:51:05.702735Z",
     "shell.execute_reply": "2024-12-28T01:51:05.701688Z",
     "shell.execute_reply.started": "2024-12-28T01:51:05.695208Z"
    },
    "trusted": true
   },
   "source": [
    "# 超参修改\n",
    "seeds = [0,0,0,0,0]\n",
    "# seeds = [0,0,0,0,0]\n",
    "eval_seed = 0\n",
    "\n",
    "train_augmentations = [\n",
    "    torchvision.transforms.Compose([\n",
    "        rand_augment_transform(\n",
    "            config_str='rand-m7-mstd0.5',\n",
    "            hparams=dict()\n",
    "        ) \n",
    "    ]),\n",
    "    \n",
    "    torchvision.transforms.Compose([\n",
    "        torchvision.transforms.ToTensor(), \n",
    "        torchvision.transforms.RandomErasing(p=0.25),\n",
    "        torchvision.transforms.ToPILImage()\n",
    "    ]),\n",
    "    \n",
    "    torchvision.transforms.Compose([\n",
    "        torchvision.transforms.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2, hue=0.1),\n",
    "    ]),\n",
    "    \n",
    "    torchvision.transforms.Compose([\n",
    "        torchvision.transforms.Pad(4),                  # Padding\n",
    "        torchvision.transforms.RandomResizedCrop(32),  # 裁剪并resize\n",
    "    ]),\n",
    "    \n",
    "    torchvision.transforms.Compose([\n",
    "        torchvision.transforms.RandomHorizontalFlip(p=0.5),          # 随机水平翻转\n",
    "    ]),\n",
    "]\n",
    "\n",
    "# train_aug_config = [{},{},{},{\"mixup_alpha\":0.1},{\"cutmix_alpha\":1.0}]\n",
    "train_aug_config = [{},{},{},{},{}]\n",
    "if_aug_matrix = False\n",
    "aug_matrix_train_aug_combines = []\n",
    "aug_matrix_val_results = []\n",
    "if if_aug_matrix:\n",
    "    temp_aug_config = []\n",
    "    for i in range(len(train_augmentations)):\n",
    "        for j in range(len(train_augmentations)):\n",
    "            if i == j:\n",
    "                aug_matrix_train_aug_combines.append(torchvision.transforms.Compose([train_augmentations[i]]))\n",
    "            else:\n",
    "                aug_matrix_train_aug_combines.append(torchvision.transforms.Compose([train_augmentations[i],train_augmentations[j]]))\n",
    "            temp_aug_config.append(train_aug_config[i] | train_aug_config[j])\n",
    "    train_aug_config = temp_aug_config\n",
    "# 超参\n",
    "use_one_model = False\n",
    "\n",
    "num_epochs = 90\n",
    "learning_rate = 0.001\n",
    "batch_size = 128\n",
    "weight_decay = 0\n",
    "grad_clip  = 0\n",
    "save_bin_name = \"202501101029\"\n",
    "datasets = \"uoft-cs/cifar10\"\n",
    "datasets_image_column_name = \"img\"\n",
    "number_classes = 10\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "pretrain_model_list = []\n",
    "model_list = []\n",
    "\n",
    "model_name1 = \"microsoft/resnet-50\"\n",
    "model1 = AutoModelForImageClassification.from_pretrained(model_name1, trust_remote_code=True)\n",
    "pretrain_model_list.append(model1)\n",
    "model_list.append(\"acc_best_model_202412300900.bin\")\n",
    "\n",
    "model_name2 = \"leftthomas/resnet50\"\n",
    "model2 = AutoModelForImageClassification.from_pretrained(model_name2, trust_remote_code=True)\n",
    "pretrain_model_list.append(model2)\n",
    "model_list.append(\"acc_best_model_202412301709.bin\")\n",
    "\n",
    "model3 = torch.hub.load('pytorch/vision:v0.10.0', 'resnet50', weights=ResNet50_Weights.IMAGENET1K_V1)\n",
    "pretrain_model_list.append(model3)\n",
    "model_list.append(\"acc_best_model_202501052203-0.bin\")\n",
    "\n",
    "model4 = torch.hub.load('pytorch/vision:v0.10.0', 'resnet50', weights=ResNet50_Weights.IMAGENET1K_V2)\n",
    "pretrain_model_list.append(model4)\n",
    "model_list.append(\"acc_best_model_202501052203-1.bin\")\n",
    "\n",
    "model5 = torch.hub.load('NVIDIA/DeepLearningExamples:torchhub', 'nvidia_resnet50', pretrained=True)\n",
    "pretrain_model_list.append(model5)\n",
    "model_list.append(\"acc_best_model_202412302006.bin\")\n",
    "\n",
    "if use_one_model:\n",
    "    number_models = len(pretrain_model_list)\n",
    "    pretrain_model_list = []\n",
    "    for i in range(number_models):\n",
    "        pretrain_model_list.append(model1)\n",
    "\n",
    "for i,model in enumerate(pretrain_model_list):\n",
    "\n",
    "    # 修改分类头\n",
    "    if not hasattr(model,\"fc\") and hasattr(model, \"classifier\"):\n",
    "        model.classifier = nn.Sequential(\n",
    "            model.classifier[0],\n",
    "            nn.Linear(in_features=2048, out_features=number_classes, bias=True)\n",
    "        )\n",
    "    elif hasattr(model, \"model\") and hasattr(model.model, \"fc\"):\n",
    "        model.model.fc = nn.Linear(in_features=2048, out_features=number_classes, bias=True)\n",
    "    elif hasattr(model,\"fc\"):\n",
    "        model.fc = nn.Linear(in_features=2048, out_features=number_classes, bias=True)\n",
    "    \n",
    "    model.to(device)\n",
    "    if is_eval:\n",
    "        model.load_state_dict(torch.load(model_list[i]))\n",
    "        print(f\"{i}th Model loaded from {model_list[i]}\")\n",
    "    \n",
    "\n",
    "use_adam = True\n",
    "\n",
    "use_sgd = False\n",
    "sgd_momentum = 0.9\n",
    "sgd_nesterov = False\n",
    "\n",
    "use_custom_scheduler = False\n",
    "custom_scheduler_gammas = [0.1, 0.01, 0.001, 0.0005]\n",
    "custom_scheduler_milestones = [80, 120,160,180]\n",
    "\n",
    "use_cosine_annealing = False\n",
    "\n",
    "use_onecyclelr = False\n",
    "\n",
    "use_reducelronplateau = False\n",
    "reducelronplateau_mode = \"max\"\n",
    "reducelronplateau_factor = 0.1\n",
    "reducelronplateau_patience = 3\n",
    "reducelronplateau_threshold = 0.001\n",
    "\n",
    "use_steplr = True\n",
    "steplr_step_size = 30\n",
    "steplr_gamma = 0.1\n",
    "\n",
    "\n",
    "test_augmentation_transforms = torchvision.transforms.Compose([\n",
    "    # torchvision.transforms.Pad(4),                                # Padding\n",
    "    # torchvision.transforms.RandomCrop(32, padding=4),\n",
    "    # torchvision.transforms.RandomCrop(32, padding=4, padding_mode='reflect'),\n",
    "    # torchvision.transforms.Resize((160,160)),\n",
    "    # torchvision.transforms.RandomCrop((128,128)),\n",
    "    # torchvision.transforms.RandomCrop(32),                       # 随机裁剪\n",
    "    # torchvision.transforms.RandomHorizontalFlip(p=0.5),          # 随机水平翻转\n",
    "    # torchvision.transforms.RandomResizedCrop(32),  # 裁剪并resize\n",
    "    # torchvision.transforms.ColorJitter(brightness=0.1, contrast=0.1, saturation=0.1, hue=0.05),\n",
    "    # torchvision.transforms.Resize((int(32/0.95),int(32/0.95))),\n",
    "    # torchvision.transforms.CenterCrop((32, 32)),\n",
    "    # torchvision.transforms.RandomAdjustSharpness(2, p=0.5)\n",
    "    # torchvision.transforms.RandomCrop((32,32)),\n",
    "    # torchvision.transforms.RandomHorizontalFlip(p=0.5),\n",
    "    # torchvision.transforms.RandomCrop(32, padding=4),  # 随机裁剪\n",
    "])\n",
    "\n",
    "# pre_get_mean = [0.4214581847190857, 0.3764420747756958, 0.28500789403915405] \n",
    "# pre_get_std = [0.293678343296051, 0.24473334848880768, 0.27143558859825134]\n",
    "pre_get_mean = []\n",
    "pre_get_std = []\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-28T01:51:09.145399Z",
     "iopub.status.busy": "2024-12-28T01:51:09.144982Z",
     "iopub.status.idle": "2024-12-28T01:51:09.152443Z",
     "shell.execute_reply": "2024-12-28T01:51:09.151586Z",
     "shell.execute_reply.started": "2024-12-28T01:51:09.145366Z"
    },
    "trusted": true
   },
   "source": [
    "def set_random_seed(seed):\n",
    "    \"\"\"\n",
    "    设置随机种子以确保实验的可重复性\n",
    "    \"\"\"\n",
    "    # 设置 PyTorch 的随机种子\n",
    "    torch.manual_seed(seed)\n",
    "    # 如果使用 GPU，也需要设置随机种子\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)  # 如果有多个 GPU\n",
    "    # 设置 NumPy 的随机种子\n",
    "    np.random.seed(seed)\n",
    "    # 设置 Python 内置的随机数生成器的种子\n",
    "    random.seed(seed)\n",
    "    # 确保 PyTorch 的随机性是可重复的\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False  # 关闭自动优化\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "def calculate_mean_std(dataset, batch_size=1):\n",
    "    \"\"\"\n",
    "    动态计算数据集的均值和标准差，适用于任何数据集。\n",
    "    Args:\n",
    "        dataset (torch.utils.data.Dataset): 目标数据集。\n",
    "        batch_size (int): 数据加载的批量大小（默认64）。\n",
    "    Returns:\n",
    "        mean (list): 每个通道的均值。\n",
    "        std (list): 每个通道的标准差。\n",
    "    \"\"\"\n",
    "    loader = DataLoader(dataset, batch_size=batch_size, shuffle=False, num_workers=4)\n",
    "    # 用于存储累计的均值和方差\n",
    "    total_sum = 0.0\n",
    "    total_squared_sum = 0.0\n",
    "    total_pixels = 0\n",
    "    # 遍历数据集，计算总和和平方和\n",
    "    for images, _ in loader:\n",
    "        # 图像形状为 (batch_size, channels, height, width)\n",
    "        batch_samples = images.size(0)  # 当前批次的样本数量\n",
    "        pixels_per_image = images.size(2) * images.size(3)  # 每张图片的像素数\n",
    "        total_pixels += batch_samples * pixels_per_image\n",
    "        # 将图像展开为 (batch_size, channels, -1) 后求和\n",
    "        total_sum += images.sum(dim=[0, 2, 3])  # 每个通道的总和\n",
    "        total_squared_sum += (images ** 2).sum(dim=[0, 2, 3])  # 每个通道的平方和\n",
    "    # 计算均值和标准差\n",
    "    mean = total_sum / total_pixels\n",
    "    std = torch.sqrt((total_squared_sum / total_pixels) - (mean ** 2))\n",
    "    return mean.tolist(), std.tolist()\n"
   ],
   "id": "e49cd553101468a9",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "e42047861e8385fb",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-28T01:51:11.561644Z",
     "iopub.status.busy": "2024-12-28T01:51:11.561345Z",
     "iopub.status.idle": "2024-12-28T01:51:11.570420Z",
     "shell.execute_reply": "2024-12-28T01:51:11.569537Z",
     "shell.execute_reply.started": "2024-12-28T01:51:11.561622Z"
    },
    "trusted": true
   },
   "source": [
    "# Training function\n",
    "def train(model, train_loader, optimizer, criterion, device, train_aug_config):\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    for images, labels in tqdm(train_loader, desc=\"Training\", leave=False):\n",
    "        images, labels = images.to(device), labels.to(device)\n",
    "\n",
    "        if hasattr(train_aug_config,\"mixup_alpha\") or hasattr(train_aug_config,\"cutmix_alpha\"):\n",
    "            mixup_alpha = train_aug_config[\"mixup_alpha\"] if hasattr(train_aug_config,\"mixup_alpha\") else 0\n",
    "            cutmix_alpha = train_aug_config[\"cutmix_alpha\"] if hasattr(train_aug_config,\"cutmix_alpha\") else 0\n",
    "            # 初始化 Mixup\n",
    "            mixup_fn = timm.data.mixup.Mixup(\n",
    "                mixup_alpha= mixup_alpha,  # Mixup 的 alpha 参数\n",
    "                cutmix_alpha= cutmix_alpha, # 如果不使用 CutMix，可以将其设为 0\n",
    "                label_smoothing=0.0,  # 标签平滑，如果需要\n",
    "                num_classes=number_classes    # 数据集的类别数量\n",
    "            )\n",
    "            images, labels = mixup_fn(images, labels) \n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        if hasattr(model(images), \"logits\"):\n",
    "                outputs = model(images).logits\n",
    "        elif isinstance(model(images), dict):\n",
    "            outputs = model(images)[\"logits\"]\n",
    "        else:\n",
    "            outputs = model(images)\n",
    "        \n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "\n",
    "        if grad_clip != 0:\n",
    "            # 裁剪梯度的范数\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=grad_clip)\n",
    "        \n",
    "        optimizer.step()\n",
    "        running_loss += loss.item()\n",
    "    return running_loss / len(train_loader)\n",
    "\n",
    "def evaluate_model(model, data_loader, criterion, device, need_record=False):\n",
    "    \"\"\"\n",
    "    评估模型并记录所有样本的最大信心分数，同时区分正确分类和错误分类的样本。\n",
    "\n",
    "    Args:\n",
    "        model: 评估的模型。\n",
    "        data_loader: 数据加载器（测试集或验证集）。\n",
    "        criterion: 损失函数。\n",
    "        device: 设备（CPU/GPU）。\n",
    "\n",
    "    Returns:\n",
    "        avg_loss: 平均损失。\n",
    "        accuracy: 准确率。\n",
    "        correct_confidences: 正确分类样本的最大信心分数列表。\n",
    "        wrong_confidences: 错误分类样本的最大信心分数列表。\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    total_loss = 0.0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    correct_confidences = []\n",
    "    wrong_confidences = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for images, labels in tqdm(data_loader, desc=\"Evaluating\", leave=False):\n",
    "            images, labels = images.to(device), labels.to(device)\n",
    "            if hasattr(model(images), \"logits\"):\n",
    "                outputs = model(images).logits\n",
    "            elif isinstance(model(images), dict):\n",
    "                outputs = model(images)[\"logits\"]\n",
    "            else:\n",
    "                outputs = model(images)\n",
    "            \n",
    "            loss = criterion(outputs, labels)\n",
    "\n",
    "            total_loss += loss.item()\n",
    "            probabilities = torch.softmax(outputs, dim=1)\n",
    "            max_confidences, predicted = probabilities.max(1)\n",
    "            \n",
    "            total += labels.size(0)\n",
    "            correct += predicted.eq(labels).sum().item()\n",
    "            \n",
    "            if need_record:\n",
    "                # 根据分类正确与否记录最大信心分数\n",
    "                for i in range(len(labels)):\n",
    "                    if predicted[i] == labels[i]:\n",
    "                        correct_confidences.append(max_confidences[i].item())\n",
    "                    else:\n",
    "                        wrong_confidences.append(max_confidences[i].item())\n",
    "\n",
    "    accuracy = 100. * correct / total\n",
    "\n",
    "    return total_loss / len(data_loader), accuracy, correct_confidences, wrong_confidences\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "def get_remote_predict(image,test_mean,test_std):\n",
    "    \n",
    "    print(\"start request remote model.\")\n",
    "    start_time = datetime.now()\n",
    "    \n",
    "    image = image.cpu()\n",
    "    # 对图像进行反归一化\n",
    "    denormalized_image = denormalize(image.unsqueeze(0), test_mean, test_std).squeeze(0).clamp(0, 1)\n",
    "\n",
    "    # 转换图像为PIL格式并保存\n",
    "    transform = torchvision.transforms.ToPILImage()\n",
    "    pil_image = transform(denormalized_image)\n",
    "    \n",
    "    # 将PIL图像转换为字节流\n",
    "    image_bytes = io.BytesIO()\n",
    "    pil_image.save(image_bytes, format='PNG')\n",
    "    image_bytes.seek(0)\n",
    "    \n",
    "    client = OpenAI(\n",
    "            base_url= openai_api_rul,\n",
    "            api_key= openai_api_key,\n",
    "    )\n",
    "    \n",
    "    result = \"\"\n",
    "    \n",
    "    messages = [\n",
    "        {\n",
    "            \"role\": \"system\", \n",
    "            \"content\": \"Please analyze the given image and determine which category it belongs to from the following list: Airplane, Automobile, Bird, Cat, Deer, Dog, Frog, Horse, Ship, Truck. Respond with only the index number corresponding to the category, where the indexes are as follows: 0: Airplane; 1: Automobile; 2: Bird; 3: Cat; 4: Deer; 5: Dog; 6: Frog; 7: Horse; 8: Ship; 9: Truck. Provide ONLY the index number as your answer. DO NOT add more text.\"\n",
    "        },\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": [\n",
    "                {\n",
    "                    \"type\": \"text\", \"text\": \"What is the category of the image? Please analyze the given image and determine which category it belongs to from the following list: Airplane, Automobile, Bird, Cat, Deer, Dog, Frog, Horse, Ship, Truck. Respond with only the index number corresponding to the category, where the indexes are as follows: 0: Airplane; 1: Automobile; 2: Bird; 3: Cat; 4: Deer; 5: Dog; 6: Frog; 7: Horse; 8: Ship; 9: Truck. Provide ONLY the index number as your answer. DO NOT add more text.\"\n",
    "                },\n",
    "                {\n",
    "                    \"type\": \"image_url\",\n",
    "                    \"image_url\": {\"url\": f\"data:image/jpg;base64,{base64.b64encode(image_bytes.read()).decode('utf-8')}\"},\n",
    "                },\n",
    "            ]\n",
    "        }\n",
    "    ]\n",
    "    \n",
    "    while True:  # 无限循环直到成功\n",
    "        try:\n",
    "            chat_completion = client.chat.completions.create(\n",
    "                messages=messages,\n",
    "                model=\"gpt-4o\",\n",
    "            )\n",
    "            print(chat_completion.choices[0].message.content)\n",
    "            result = chat_completion.choices[0].message.content\n",
    "            break  # 如果请求成功，退出循环\n",
    "        except Exception as e:\n",
    "            error = str(e)\n",
    "            if \"Cloudflare\" not in error:\n",
    "                # 如果不是 Cloudflare 错误，直接打印并退出\n",
    "                print(error)\n",
    "                break\n",
    "            else:\n",
    "                # 如果是 Cloudflare 错误，等待 3 秒后重试\n",
    "                print(\"遇到 Cloudflare 错误，等待 3 秒后重试...\")\n",
    "                time.sleep(3)\n",
    "    \n",
    "    end_time = datetime.now()\n",
    "    elapsed_time = (end_time - start_time).total_seconds()\n",
    "    print(f\"请求远程模型花费时间: {elapsed_time} 秒\")\n",
    "    return result, elapsed_time"
   ],
   "id": "1fc9403401facad8",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "def ensemble_evaluate_model(ensemble_model_list, data_loader, criterion, device,need_record=False, remote_threshold=1.0,test_mean=None,test_std=None):\n",
    "    for model in ensemble_model_list:\n",
    "        model.eval()\n",
    "\n",
    "    total_loss = 0.0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    confidences = []\n",
    "    is_correct = []\n",
    "    total_time = 0\n",
    "    local_predict_time = 0\n",
    "    remote_predict_time = 0\n",
    "    remote_predict_count = 0\n",
    "    remote_reject_count = 0\n",
    "    remote_correct_count = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for images, labels in tqdm(data_loader, desc=\"Evaluating\", leave=False):\n",
    "            images, labels = images.to(device), labels.to(device)\n",
    "            output_list = []\n",
    "            start_time = datetime.now()\n",
    "            for model in ensemble_model_list:\n",
    "                if hasattr(model(images), \"logits\"):\n",
    "                    outputs = model(images).logits\n",
    "                elif isinstance(model(images), dict):\n",
    "                    outputs = model(images)[\"logits\"]\n",
    "                else:\n",
    "                    outputs = model(images)\n",
    "                output_list.append(outputs)\n",
    "            stacked_outputs = torch.stack(output_list)\n",
    "            outputs = torch.mean(stacked_outputs, dim=0)\n",
    "            \n",
    "            loss = criterion(outputs, labels)\n",
    "\n",
    "            total_loss += loss.item()\n",
    "            probabilities = torch.softmax(outputs, dim=1)\n",
    "            max_confidences, predicted = probabilities.max(1)\n",
    "            \n",
    "            end_time = datetime.now()\n",
    "            elapsed_time = (end_time - start_time).total_seconds()\n",
    "            total_time += elapsed_time\n",
    "            local_predict_time += elapsed_time\n",
    "            if 0.0 <= remote_threshold <= 1.0:\n",
    "                masks = max_confidences < remote_threshold\n",
    "                for i,mask in enumerate(masks):\n",
    "                    if mask:\n",
    "                        remote_predict,time_spend = get_remote_predict(images[i],test_mean,test_std)\n",
    "                        remote_predict_time += time_spend\n",
    "                        total_time += time_spend\n",
    "                        remote_predict_count += 1\n",
    "                        try:\n",
    "                            predicted[i] = int(remote_predict)\n",
    "                            if predicted[i] == labels[i]:\n",
    "                                remote_correct_count += 1\n",
    "                        except Exception as e:\n",
    "                            remote_reject_count += 1\n",
    "                            remote_predict_count -= 1\n",
    "            \n",
    "            total += labels.size(0)\n",
    "            correct += predicted.eq(labels).sum().item()\n",
    "            \n",
    "            if need_record:\n",
    "                # 保存置信度和分类正确性\n",
    "                confidences.extend(max_confidences.cpu().numpy())\n",
    "                is_correct.extend(predicted.eq(labels).cpu().numpy())\n",
    "\n",
    "    print(\"total avg time:\",total_time/len(data_loader.dataset))        \n",
    "    print(\"total count:\",len(data_loader.dataset))\n",
    "    print(\"local avg time:\",local_predict_time/len(data_loader.dataset))    \n",
    "    if remote_predict_count+remote_reject_count > 0:\n",
    "        print(\"remote avg time:\",remote_predict_time/(remote_predict_count+remote_reject_count))\n",
    "    print(\"remote_predict_count:\",remote_predict_count)\n",
    "    print(\"remote_reject_count:\",remote_reject_count)\n",
    "    print(\"remote_correct_count:\",remote_correct_count)\n",
    "    print(\"total correct:\", correct)\n",
    "    accuracy = 100. * correct / total\n",
    "\n",
    "    return total_loss / len(data_loader), accuracy, np.array(confidences), np.array(is_correct)\n"
   ],
   "id": "39f159aa7c3a41cb",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "def save_image(path,name,image,test_mean,test_std):\n",
    "    image = image.cpu()\n",
    "    # 对图像进行反归一化\n",
    "    denormalized_image = denormalize(image.unsqueeze(0), test_mean, test_std).squeeze(0).clamp(0, 1)\n",
    "\n",
    "    # 转换图像为PIL格式并保存\n",
    "    transform = torchvision.transforms.ToPILImage()\n",
    "    pil_image = transform(denormalized_image)\n",
    "    save_path = path\n",
    "    save_name = name\n",
    "    pil_image.save(save_path+\"/\"+save_name)"
   ],
   "id": "9123526c81d60c27",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "9f66fa45ba7ad354",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-28T01:51:14.011543Z",
     "iopub.status.busy": "2024-12-28T01:51:14.011256Z",
     "iopub.status.idle": "2024-12-28T01:51:14.019398Z",
     "shell.execute_reply": "2024-12-28T01:51:14.018513Z",
     "shell.execute_reply.started": "2024-12-28T01:51:14.011520Z"
    },
    "trusted": true
   },
   "source": [
    "def get_dataloaders(augmentation_transforms,val_augmentation_transforms, batch_size=128):\n",
    "    # Load dataset from Hugging Face\n",
    "    dataset = load_dataset(datasets)\n",
    "    transform = torchvision.transforms.Compose([\n",
    "        torchvision.transforms.ToTensor(),  # 将 PIL.Image 转换为 Tensor\n",
    "    ])\n",
    "\n",
    "    # 定义训练集和验证集的划分比例\n",
    "    train_size = int(0.8 * len(dataset['train']))  # 80% 作为训练集\n",
    "    val_size = len(dataset['train']) - train_size  # 剩下的作为验证集\n",
    "    train_subset, val_subset = random_split(dataset['train'], [train_size, val_size])\n",
    "\n",
    "    # 转换为 Tensor 格式\n",
    "    train_dataset = [(transform(item[datasets_image_column_name].convert(\"RGB\")), item[\"label\"]) for item in train_subset]\n",
    " \n",
    "    if len(pre_get_mean) > 0 and len(pre_get_std) > 0:\n",
    "        mean = pre_get_mean\n",
    "        std = pre_get_std\n",
    "    else:\n",
    "        # **只使用训练集计算均值和标准差**\n",
    "        mean, std = calculate_mean_std(train_dataset)\n",
    "        print(\"Train dataset mean and std:\", mean, std)\n",
    "\n",
    "    # 定义训练集的 transform（包括数据增强）\n",
    "    train_transform = torchvision.transforms.Compose([\n",
    "        augmentation_transforms,\n",
    "        torchvision.transforms.ToTensor(),\n",
    "        torchvision.transforms.Normalize(mean=mean, std=std)\n",
    "    ])\n",
    "\n",
    "    # 验证集和测试集的 transform（不包括数据增强）\n",
    "    eval_transform = torchvision.transforms.Compose([\n",
    "        val_augmentation_transforms,\n",
    "        torchvision.transforms.ToTensor(),\n",
    "        torchvision.transforms.Normalize(mean=mean, std=std)\n",
    "    ])\n",
    "\n",
    "    # 应用 transforms\n",
    "    train_dataset = [(train_transform(item[datasets_image_column_name].convert(\"RGB\")), item[\"label\"]) for item in train_subset]\n",
    "    val_dataset = [(eval_transform(item[datasets_image_column_name].convert(\"RGB\")), item[\"label\"]) for item in val_subset]\n",
    "\n",
    "    # 创建 DataLoader\n",
    "    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, num_workers=4)\n",
    "    val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False, num_workers=4)\n",
    "\n",
    "    return train_loader, val_loader"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "def get_test_loader(test_augmentation_transforms, batch_size=128,size=10000):\n",
    "    \n",
    "    # Load dataset from Hugging Face\n",
    "    dataset = load_dataset(datasets)\n",
    "    transform = torchvision.transforms.Compose([\n",
    "        torchvision.transforms.ToTensor(),  # 将 PIL.Image 转换为 Tensor\n",
    "    ])\n",
    "\n",
    "    # 定义训练集和验证集的划分比例\n",
    "    train_size = int(0.8 * len(dataset['train']))  # 80% 作为训练集\n",
    "    val_size = len(dataset['train']) - train_size  # 剩下的作为验证集\n",
    "    train_subset, val_subset = random_split(dataset['train'], [train_size, val_size])\n",
    "\n",
    "    # 转换为 Tensor 格式\n",
    "    train_dataset = [(transform(item[datasets_image_column_name].convert(\"RGB\")), item[\"label\"]) for item in train_subset]\n",
    " \n",
    "    if len(pre_get_mean) > 0 and len(pre_get_std) > 0:\n",
    "        mean = pre_get_mean\n",
    "        std = pre_get_std\n",
    "    else:\n",
    "        # **只使用训练集计算均值和标准差**\n",
    "        mean, std = calculate_mean_std(train_dataset)\n",
    "        print(\"Train dataset mean and std:\", mean, std)\n",
    "\n",
    "    # 验证集和测试集的 transform（不包括数据增强）\n",
    "    eval_transform = torchvision.transforms.Compose([\n",
    "        test_augmentation_transforms,\n",
    "        torchvision.transforms.ToTensor(),\n",
    "        torchvision.transforms.Normalize(mean=mean, std=std)\n",
    "    ])\n",
    "\n",
    "    test_dataset = [(eval_transform(item[datasets_image_column_name].convert(\"RGB\")), item[\"label\"]) for item in dataset[\"test\"]]\n",
    "    \n",
    "    if size > len(test_dataset):\n",
    "        raise ValueError(\"测试集子集大小不能超过测试集大小\")\n",
    "    \n",
    "    subset_indices = random.sample(range(len(test_dataset)), size)\n",
    "    \n",
    "    test_subset = Subset(test_dataset, subset_indices)\n",
    "\n",
    "    test_loader = DataLoader(test_subset, batch_size=batch_size, shuffle=False, num_workers=4)\n",
    "\n",
    "    return test_loader, mean, std"
   ],
   "id": "e1b245bb70042ca8",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "def visualize_confidences(correct_confidences, wrong_confidences):\n",
    "    \"\"\"\n",
    "    可视化正确分类样本和错误分类样本的最大信心分数。\n",
    "\n",
    "    Args:\n",
    "        correct_confidences: 正确分类样本的最大信心分数列表。\n",
    "        wrong_confidences: 错误分类样本的最大信心分数列表。\n",
    "    \"\"\"\n",
    "    plt.figure(figsize=(number_classes, 6))\n",
    "    plt.hist(correct_confidences, bins=50, alpha=0.7, label='Correctly Classified', color='green')\n",
    "    plt.hist(wrong_confidences, bins=50, alpha=0.7, label='Misclassified', color='orange')\n",
    "    plt.xlabel('Maximum Confidence Score')\n",
    "    plt.ylabel('Frequency')\n",
    "    plt.title('Confidence Distribution of Correct and Wrong Classifications')\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    plt.show()"
   ],
   "id": "b5682dc4acd5d2be",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "class CustomMultiStepLR(_LRScheduler):\n",
    "    def __init__(self, optimizer, milestones, gammas, last_epoch=-1):\n",
    "        self.milestones = milestones\n",
    "        self.gammas = gammas\n",
    "        assert len(milestones) == len(gammas), \"Milestones and gammas must have the same length\"\n",
    "        super(CustomMultiStepLR, self).__init__(optimizer, last_epoch)\n",
    "\n",
    "    def get_lr(self):\n",
    "        factor = 1.0\n",
    "        for milestone, gamma in zip(self.milestones, self.gammas):\n",
    "            if self.last_epoch >= milestone:\n",
    "                factor = gamma  # 只应用当前 milestone 的 gamma\n",
    "        return [base_lr * factor for base_lr in self.base_lrs]"
   ],
   "id": "4f5ba3ef8f60c1e6",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# 定义反归一化函数\n",
    "def denormalize(tensor, mean, std):\n",
    "    \"\"\"\n",
    "    将归一化的张量还原为原始像素值范围\n",
    "    :param tensor: 标准化后的张量\n",
    "    :param mean: 均值\n",
    "    :param std: 标准差\n",
    "    :return: 非归一化的张量\n",
    "    \"\"\"\n",
    "    mean = torch.tensor(mean).view(1, -1, 1, 1)\n",
    "    std = torch.tensor(std).view(1, -1, 1, 1)\n",
    "    return tensor * std + mean"
   ],
   "id": "3b5996b43e20441f",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "def plot_confidence_accuracy_and_ece(confidences, is_correct, num_bins=10):\n",
    "    \"\"\"\n",
    "    绘制置信度与准确率、样本数量的关系曲线，并计算 ECE\n",
    "    :param confidences: 每个样本的最大置信度\n",
    "    :param is_correct: 每个样本的分类正确性 (1: 正确, 0: 错误)\n",
    "    :param num_bins: 分箱数量，用于 ECE 计算\n",
    "    \"\"\"\n",
    "    thresholds = np.linspace(0.0, 1.0, 50)  # 设置不同的置信度阈值\n",
    "    accuracies = []\n",
    "    sample_counts = []\n",
    "\n",
    "    for tau in thresholds:\n",
    "        mask = confidences >= tau  # 筛选置信度大于 tau 的样本\n",
    "        if mask.sum() > 0:  # 如果筛选后的样本数大于0\n",
    "            accuracy = is_correct[mask].mean()  # 计算准确率\n",
    "        else:\n",
    "            accuracy = 0.0  # 无样本时准确率为0\n",
    "        accuracies.append(accuracy)\n",
    "        sample_counts.append(mask.sum())\n",
    "    \n",
    "    # 计算 ECE\n",
    "    bin_boundaries = np.linspace(0.0, 1.0, num_bins + 1)  # 分箱边界\n",
    "    ece = 0.0\n",
    "    bin_accuracies = []\n",
    "    bin_confidences = []\n",
    "    bin_sample_counts = []\n",
    "\n",
    "    for i in range(num_bins):\n",
    "        bin_lower = bin_boundaries[i]\n",
    "        bin_upper = bin_boundaries[i + 1]\n",
    "        bin_mask = (confidences > bin_lower) & (confidences <= bin_upper)\n",
    "        bin_size = bin_mask.sum()\n",
    "\n",
    "        if bin_size > 0:\n",
    "            bin_accuracy = is_correct[bin_mask].mean()\n",
    "            bin_confidence = confidences[bin_mask].mean()\n",
    "            ece += (bin_size / len(confidences)) * abs(bin_accuracy - bin_confidence)\n",
    "\n",
    "            bin_accuracies.append(bin_accuracy)\n",
    "            bin_confidences.append(bin_confidence)\n",
    "            bin_sample_counts.append(bin_size)\n",
    "        else:\n",
    "            bin_accuracies.append(0.0)\n",
    "            bin_confidences.append(0.0)\n",
    "            bin_sample_counts.append(0)\n",
    "\n",
    "    # 绘制曲线\n",
    "    plt.figure(figsize=(10, 8))\n",
    "\n",
    "    # 绘制准确率和样本数量曲线\n",
    "    plt.subplot(2, 1, 1)\n",
    "    plt.plot(thresholds, accuracies, label=\"Accuracy vs. Confidence Threshold\", color=\"b\")\n",
    "    plt.xlabel(\"Confidence Threshold (τ)\")\n",
    "    plt.ylabel(\"Accuracy\")\n",
    "    plt.title(\"Accuracy vs. Confidence Threshold\")\n",
    "    plt.grid(True)\n",
    "    plt.legend()\n",
    "\n",
    "    plt.subplot(2, 1, 2)\n",
    "    plt.plot(thresholds, sample_counts, label=\"Sample Count vs. Confidence Threshold\", color=\"g\")\n",
    "    plt.xlabel(\"Confidence Threshold (τ)\")\n",
    "    plt.ylabel(\"Sample Count\")\n",
    "    plt.title(\"Sample Count vs. Confidence Threshold\")\n",
    "    plt.grid(True)\n",
    "    plt.legend()\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "    return ece, bin_accuracies, bin_confidences, bin_sample_counts\n"
   ],
   "id": "cf1a32c89cfe9c76",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "def evaluate_score_vs_threshold(confidences, is_correct, alpha=0.5, beta=0.5):\n",
    "    \"\"\"\n",
    "    计算不同置信度阈值下的准确率、覆盖率和得分\n",
    "    :param confidences: 每个样本的最大置信度\n",
    "    :param is_correct: 每个样本的分类正确性 (1: 正确, 0: 错误)\n",
    "    :param alpha: 准确率的权重\n",
    "    :param beta: 覆盖率的权重\n",
    "    :return: thresholds, accuracies, coverages, scores\n",
    "    \"\"\"\n",
    "    thresholds = np.linspace(0.0, 1.0, 50)\n",
    "    accuracies = []\n",
    "    coverages = []\n",
    "    scores = []\n",
    "\n",
    "    total_samples = len(confidences)\n",
    "\n",
    "    for tau in thresholds:\n",
    "        mask = confidences >= tau\n",
    "        \n",
    "        if mask.sum() > 0:\n",
    "            accuracy = is_correct[mask].mean()  # 准确率\n",
    "            coverage = mask.sum() / total_samples  # 覆盖率\n",
    "        else:\n",
    "            accuracy = 0.0\n",
    "            coverage = 0.0\n",
    "        \n",
    "        if alpha + beta > 1.0:\n",
    "            beta = 1 - alpha\n",
    "            if beta >= 0 and beta <= 1.0:\n",
    "                print(f\"beta has been justified to {beta}\")\n",
    "            else:\n",
    "                alpha = 0.5\n",
    "                beta = 0.5\n",
    "                print(f\"alpha has been justified to {alpha}\")\n",
    "                print(f\"beta has been justified to {beta}\")\n",
    "        \n",
    "        gamma = 1 - alpha - beta\n",
    "        \n",
    "        # 计算得分\n",
    "        score  = alpha * accuracy + beta * coverage + gamma * accuracy * coverage\n",
    "        \n",
    "        accuracies.append(accuracy)\n",
    "        coverages.append(coverage)\n",
    "        scores.append(score)\n",
    "    \n",
    "    best_score_index = scores.index(max(scores))\n",
    "    print(\"Threshold of the best score:\", thresholds[best_score_index])\n",
    "    print(\"Accuracy of the best score:\", accuracies[best_score_index])\n",
    "    print(\"Coverage of the best score:\", coverages[best_score_index])\n",
    "    return thresholds, accuracies, coverages, scores\n",
    "\n",
    "def plot_score_vs_threshold(thresholds, accuracies, coverages, scores):\n",
    "    \"\"\"\n",
    "    绘制准确率、覆盖率和得分随置信度阈值变化的曲线\n",
    "    \"\"\"\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.plot(thresholds, accuracies, label=\"Accuracy\", color=\"blue\")\n",
    "    plt.plot(thresholds, coverages, label=\"Coverage\", color=\"green\")\n",
    "    plt.plot(thresholds, scores, label=\"Score\", color=\"red\", linestyle=\"--\")\n",
    "    plt.xlabel(\"Confidence Threshold (τ)\")\n",
    "    plt.ylabel(\"Value\")\n",
    "    plt.title(\"Accuracy, Coverage, and Score vs. Confidence Threshold\")\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    plt.show()"
   ],
   "id": "fb55fe207ed6ca71",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "17fa70a2f801f5c5",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-28T01:51:18.970073Z",
     "iopub.status.busy": "2024-12-28T01:51:18.969712Z",
     "iopub.status.idle": "2024-12-28T01:52:22.424961Z",
     "shell.execute_reply": "2024-12-28T01:52:22.424215Z",
     "shell.execute_reply.started": "2024-12-28T01:51:18.970036Z"
    },
    "trusted": true
   },
   "source": [
    "train_loader_list = []\n",
    "val_loader_list = []\n",
    "\n",
    "set_random_seed(eval_seed)\n",
    "test_loader,test_mean,test_std = get_test_loader(test_augmentation_transforms, batch_size=batch_size, size=1000)\n",
    "    \n",
    "if not is_eval:\n",
    "    for i,model in enumerate(pretrain_model_list):\n",
    "        set_random_seed(seeds[i])\n",
    "        if if_aug_matrix:\n",
    "            for j in range(len(aug_matrix_train_aug_combines)):\n",
    "                print(f\"aug:{aug_matrix_train_aug_combines[j]}\")\n",
    "                train_loader, val_loader = get_dataloaders(aug_matrix_train_aug_combines[j],test_augmentation_transforms,batch_size)\n",
    "                train_loader_list.append(train_loader)\n",
    "                val_loader_list.append(val_loader)\n",
    "        else:\n",
    "            train_loader, val_loader = get_dataloaders(train_augmentations[i],test_augmentation_transforms,batch_size)\n",
    "            train_loader_list.append(train_loader)\n",
    "            val_loader_list.append(val_loader)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "28579f8356786d6b",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-28T01:52:38.245785Z",
     "iopub.status.busy": "2024-12-28T01:52:38.245504Z"
    },
    "trusted": true
   },
   "source": [
    "if not is_eval:\n",
    "    for i,model in enumerate(pretrain_model_list):\n",
    "        original_state = copy.deepcopy(model.state_dict())\n",
    "        aug_times = 1\n",
    "        if if_aug_matrix and len(aug_matrix_train_aug_combines)>0:\n",
    "            aug_times = len(aug_matrix_train_aug_combines)\n",
    "        for j in range(aug_times):\n",
    "            set_random_seed(seeds[i])\n",
    "            best_val_loss = float('inf')\n",
    "            loss_best_model = None\n",
    "            best_val_acc = 0\n",
    "            acc_best_model = None\n",
    "            \n",
    "            if use_adam:\n",
    "                optimizer = optim.Adam(model.parameters(), lr=learning_rate, weight_decay=weight_decay)\n",
    "            elif use_sgd:\n",
    "                optimizer = optim.SGD(model.parameters(), lr=learning_rate, momentum=sgd_momentum, weight_decay=weight_decay, nesterov=sgd_nesterov)\n",
    "            else:\n",
    "                optimizer = optim.SGD(model.parameters(), lr=learning_rate)\n",
    "                \n",
    "            if use_custom_scheduler:\n",
    "                scheduler = CustomMultiStepLR(optimizer, custom_scheduler_milestones, custom_scheduler_gammas)\n",
    "            elif use_cosine_annealing:\n",
    "                scheduler = CosineAnnealingLR(optimizer, T_max=num_epochs)\n",
    "            elif use_onecyclelr:\n",
    "                scheduler = OneCycleLR(optimizer, learning_rate, epochs=num_epochs, steps_per_epoch=len(train_loader_list[i]))\n",
    "            elif use_reducelronplateau:\n",
    "                scheduler = ReduceLROnPlateau(optimizer, mode=reducelronplateau_mode, factor=reducelronplateau_factor, patience=reducelronplateau_patience, threshold=reducelronplateau_threshold)\n",
    "            elif use_steplr:\n",
    "                scheduler = StepLR(optimizer, step_size=steplr_step_size, gamma=steplr_gamma)\n",
    "            else:\n",
    "                scheduler = CosineAnnealingLR(optimizer, T_max=num_epochs)\n",
    "            \n",
    "            \n",
    "            for epoch in range(num_epochs):\n",
    "                print(f\"Epoch {epoch + 1}/{num_epochs}, LR: {scheduler.get_last_lr()[0]}\")\n",
    "                train_loss = train(model, train_loader_list[i*aug_times+j], optimizer, criterion, device, train_aug_config[i*aug_times+j])\n",
    "                val_loss, val_accuracy,_,_ = evaluate_model(model, val_loader_list[i*aug_times+j], criterion, device, need_record=False)\n",
    "                \n",
    "                if(val_loss <= best_val_loss):\n",
    "                    best_val_loss = val_loss\n",
    "                    loss_best_model = model.state_dict()\n",
    "                    torch.save(loss_best_model, \"loss_best_model_\"+save_bin_name+\"-\"+str(i*aug_times+j)+\".bin\")\n",
    "                    print(f\"val loss best model saved, best_val_loss: {best_val_loss}\")\n",
    "                if(val_accuracy > best_val_acc):\n",
    "                    best_val_acc = val_accuracy\n",
    "                    acc_best_model = model.state_dict()\n",
    "                    torch.save(acc_best_model, \"acc_best_model_\"+save_bin_name+\"-\"+str(i*aug_times+j)+\".bin\")\n",
    "                    print(f\"val accuracy best model saved, best_val_acc: {best_val_acc}\")\n",
    "                print(f\"Train Loss: {train_loss:.4f}, val Loss: {val_loss:.4f}, val Accuracy: {val_accuracy:.2f}%\")\n",
    "                if use_reducelronplateau:\n",
    "                    scheduler.step(metrics=val_accuracy)\n",
    "                else:\n",
    "                    scheduler.step()\n",
    "            if if_aug_matrix and len(aug_matrix_train_aug_combines)>0:\n",
    "                aug_matrix_val_results.append(best_val_acc)\n",
    "                print(aug_matrix_val_results)\n",
    "                model.load_state_dict(original_state)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "if is_eval:\n",
    "    set_random_seed(eval_seed)\n",
    "    test_loss, test_accuracy, confidences, is_correct = ensemble_evaluate_model(pretrain_model_list,test_loader, criterion, device, need_record=True,remote_threshold=0.9796,test_mean=test_mean, test_std=test_std)\n",
    "    \n",
    "    print(f\"test_loss: {test_loss}, test_accuracy: {test_accuracy}\")\n",
    "    \n",
    "    # 绘制曲线并计算 ECE\n",
    "    ece, bin_accuracies, bin_confidences, bin_sample_counts = plot_confidence_accuracy_and_ece(confidences, is_correct, num_bins=10)\n",
    "    \n",
    "    # 输出 ECE 信息\n",
    "    print(f\"Expected Calibration Error (ECE): {ece:.4f}\")\n",
    "    \n",
    "    # 打印分箱详细信息\n",
    "    print(\"\\nBin Information:\")\n",
    "    for i, (acc, conf, count) in enumerate(zip(bin_accuracies, bin_confidences, bin_sample_counts)):\n",
    "        print(f\"Bin {i + 1}: Accuracy = {acc:.4f}, Confidence = {conf:.4f}, Samples = {count}\")\n",
    "    \n",
    "\n",
    "    # 计算不同阈值下的准确率、覆盖率和得分\n",
    "    thresholds, accuracies, coverages, scores = evaluate_score_vs_threshold(\n",
    "        confidences, is_correct, alpha=0.8, beta=0.1\n",
    "    )\n",
    "    \n",
    "    # 绘制曲线\n",
    "    plot_score_vs_threshold(thresholds, accuracies, coverages, scores)"
   ],
   "id": "a62a6072238912ae",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# set_random_seed(eval_seed)\n",
    "# for i,model in enumerate(pretrain_model_list):\n",
    "#     global_correct_confidences = []\n",
    "#     global_wrong_confidences = []\n",
    "#     test_loss, test_accuracy, correct_confidences, wrong_confidences = evaluate_model(model, test_loader, criterion, device, need_record=True)\n",
    "#     global_correct_confidences.extend(correct_confidences)\n",
    "#     global_wrong_confidences.extend(wrong_confidences)\n",
    "#     print(f\"test_loss: {test_loss}, test_accuracy: {test_accuracy}\")\n",
    "#     visualize_confidences(global_correct_confidences, global_wrong_confidences)"
   ],
   "id": "2916c828c2f5075f",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "c0c263525b63dc42",
   "metadata": {
    "trusted": true
   },
   "source": [
    "# set_random_seed(eval_seed)\n",
    "# for i,model in enumerate(pretrain_model_list):\n",
    "#     global_correct_confidences = []\n",
    "#     global_wrong_confidences = []\n",
    "#     model.load_state_dict(torch.load(\"loss_best_model_\"+save_bin_name+\"-\"+str(i)+\".bin\"))\n",
    "#     test_loss, test_accuracy, correct_confidences, wrong_confidences = evaluate_model(model, test_loader, criterion, device, need_record=True)\n",
    "#     global_correct_confidences.extend(correct_confidences)\n",
    "#     global_wrong_confidences.extend(wrong_confidences)\n",
    "#     print(f\"test_loss: {test_loss}, test_accuracy: {test_accuracy}\")\n",
    "#     visualize_confidences(global_correct_confidences, global_wrong_confidences)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# set_random_seed(eval_seed)\n",
    "# for i,model in enumerate(pretrain_model_list):\n",
    "#     global_correct_confidences = []\n",
    "#     global_wrong_confidences = []\n",
    "#     model.load_state_dict(torch.load(\"acc_best_model_\"+save_bin_name+\"-\"+str(i)+\".bin\"))\n",
    "#     test_loss, test_accuracy, correct_confidences, wrong_confidences = evaluate_model(model, test_loader, criterion, device, need_record=True)\n",
    "#     global_correct_confidences.extend(correct_confidences)\n",
    "#     global_wrong_confidences.extend(wrong_confidences)\n",
    "#     print(f\"test_loss: {test_loss}, test_accuracy: {test_accuracy}\")\n",
    "#     visualize_confidences(global_correct_confidences, global_wrong_confidences)"
   ],
   "id": "146b657f46476da4",
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "gpu",
   "dataSources": [],
   "dockerImageVersionId": 30822,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
